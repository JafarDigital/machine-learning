{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "071faa07-c25f-4c12-9359-d734a56749ab",
   "metadata": {},
   "source": [
    "<h2>Dimensionality Reduction</h2>\n",
    "\n",
    "Dimensionality reduction is the process of reducing the number of features (variables) in a dataset while preserving as much relevant information as possible.\n",
    "\n",
    "It solves:\n",
    "\n",
    "    Curse of dimensionality: by Lowering feature space ‚Üí models generalize better\n",
    "\n",
    "    Overfitting: by Reducing complexity ‚Üí fewer spurious correlations\n",
    "\n",
    "    Computational inefficiency: by Speeding up training/testing\n",
    "\n",
    "    Hard-to-visualize data: by Enabling 2D or 3D plots\n",
    "\n",
    "    Multicollinearity: by Finding uncorrelated components\n",
    "\n",
    "    Noisy / redundant features: by Keeping only what matters\n",
    "\n",
    "<h3>‚öôÔ∏è Two Main Categories of Dimensionality Reduction</h3>\n",
    "\n",
    "üîπ <b>1. Feature Selection (Subset of original features)</b>\n",
    "\n",
    "    Select important features using:\n",
    "\n",
    "        Variance Threshold\n",
    "\n",
    "        Mutual Information\n",
    "\n",
    "        Recursive Feature Elimination (RFE)\n",
    "\n",
    "        SelectKBest\n",
    "\n",
    "    ‚úÖ Interpretability is preserved ‚Äî no new transformed features.\n",
    "\n",
    "üîπ <b>2. Feature Extraction (Transform to new features)</b>\n",
    "\n",
    "    Create new features that combine existing ones:\n",
    "\n",
    "        PCA (Principal Component Analysis)\n",
    "\n",
    "        t-SNE, UMAP (for visualization)\n",
    "\n",
    "        LDA (for classification)\n",
    "\n",
    "        Autoencoders (Deep Learning)\n",
    "\n",
    "<h3>üìå Golden Principles</h3>\n",
    "\n",
    "1. Don‚Äôt reduce before you understand your data. Dimensionality reduction can hide relationships and make models harder to interpret.\n",
    "\n",
    "2. Scale before you reduce. Most techniques (e.g. PCA) are sensitive to scale ‚Äî use StandardScaler.\n",
    "\n",
    "3. Keep enough variance. In PCA, you usually want to preserve 95%+ of variance.\n",
    "\n",
    "4. Use reduction after cleaning, imputation, encoding, and scaling.\n",
    "\n",
    "5. Not all techniques are for the same goal.\n",
    "\n",
    "    PCA = compression\n",
    "\n",
    "    t-SNE / UMAP = visualization\n",
    "\n",
    "    RFE = feature ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d03cdf22-cd78-4a13-80b0-0ead761c7bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a40f80-d6c6-4bdd-af3a-f75c9b248ece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
