{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fba666d-ee76-45a4-a001-f4683e7be691",
   "metadata": {},
   "source": [
    "<b>Naive Bayes classifiers</b> are a family of \"probabilistic classifiers\" which assumes that the features are conditionally independent, given the target class.\n",
    "\n",
    "In other words, a naive Bayes model assumes the information about the class provided by each variable is unrelated to the information from the others, with no information shared between the predictors. The highly unrealistic nature of this assumption, called the naive independence assumption, is what gives the classifier its name.\n",
    "\n",
    "These classifiers are some of the simplest Bayesian network models. Naive Bayes classifiers generally perform worse than more advanced models like logistic regressions, especially at quantifying uncertainty (with naive Bayes models often producing wildly overconfident probabilities). However, they are highly scalable, requiring only one parameter for each feature or predictor in a learning problem.\n",
    "\n",
    "One of the most popular use cases of Naive Bayes is spam detection which will be implemented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cea134c-bec8-48bf-87b2-fee8cda79613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/spam.csv', encoding='latin-1')[['v1', 'v2']]\n",
    "df.columns = ['label', 'text']\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72112506-6c6c-438c-8d1a-4c4309b2643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['label'].map({'ham': 0, 'spam': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a756cc4-22c8-47ce-809d-cce0cd739985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9668\n",
      "[[965   0]\n",
      " [ 37 113]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       965\n",
      "           1       1.00      0.75      0.86       150\n",
      "\n",
      "    accuracy                           0.97      1115\n",
      "   macro avg       0.98      0.88      0.92      1115\n",
      "weighted avg       0.97      0.97      0.96      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_vec, y_train)\n",
    "y_pred = model.predict(X_test_vec)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cb55e1c-9531-4987-a7d8-206881e21f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top spam words:\n",
      "['free', 'txt', 'mobile', 'claim', 'stop', 'text', 'prize', 'ur', 'reply', 'www']\n",
      "\n",
      "Top ham words:\n",
      "['ok', 'll', 'come', 'lt', 'gt', 'just', 'good', 'home', 'got', 'time']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get vocab and weights\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "log_probs = model.feature_log_prob_\n",
    "\n",
    "# Top spam words\n",
    "spam_top = np.argsort(log_probs[1])[::-1][:10]\n",
    "ham_top = np.argsort(log_probs[0])[::-1][:10]\n",
    "\n",
    "print(\"Top spam words:\")\n",
    "print([feature_names[i] for i in spam_top])\n",
    "\n",
    "print(\"\\nTop ham words:\")\n",
    "print([feature_names[i] for i in ham_top])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd885af-8aca-4d22-8881-d3ca409357ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
